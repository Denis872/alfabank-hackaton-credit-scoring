import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import KFold

RANDOM_STATE = 42

TRAIN_PATH = "/content/drive/My Drive/hackathon_income_train.csv"
TEST_PATH = "/content/drive/My Drive/hackathon_income_test.csv"
FEATURE_DESC_PATH = "features_description.csv"

TARGET_COL = "target"  # –∏–º—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π

DROP_COLS = [
    "id",
    "dt",
    TARGET_COL,
    "w"
]

WEIGHT_COL = "w"  # –≤–µ—Å–∞ –¥–ª—è WMAE

# –Ø–≤–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Ñ–∏—á–µ–π –ø–æ —Å–º—ã—Å–ª—É
EXPLICIT_CATEGORY_COLS = [
    # —Å–æ—Ü-–¥–µ–º / –≥–µ–æ
    "gender",
    "adminarea",
    "city_smart_name",
    "addrref",
    "incomeValueCategory",

    # –±–∏–Ω–∞—Ä–Ω—ã–µ —Ñ–ª–∞–≥–∏
    "blacklist_flag",
    "client_active_flag",
    "nonresident_flag",
    "accountsalary_out_flag",

    # –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π
    "vert_has_app_ru_tinkoff_investing",
    "vert_has_app_ru_vtb_invest",
    "vert_has_app_ru_cian_main",
    "vert_has_app_ru_raiffeisennews"
]

# =========================
#   –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•
# =========================

def load_data(path: str) -> pd.DataFrame:
    """
    –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —á—Ç–µ–Ω–∏–µ CSV:
    - —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å ';'
    - –¥–µ—Å—è—Ç–∏—á–Ω—ã–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å ','
    - "nan"/"NaN"/"None"/"" ‚Üí NaN
    """
    df = pd.read_csv(
        path,
        sep=";",
        decimal=".",
        on_bad_lines="skip",
    )
    df.columns = [c.strip() for c in df.columns]
    return df


def get_categorical_features(df: pd.DataFrame, feature_desc_path: str | None = None):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Ñ–∏—á.
    –ø–æ —Ç–∏–ø—É object.
    """
    cat_features = list(df.select_dtypes(include=["object", "category"]).columns)

    cat_features = [c for c in cat_features if c not in DROP_COLS and c != TARGET_COL]
    print(cat_features)
    return cat_features


def prepare_features(
    df: pd.DataFrame,
    cat_features: list[str],
    feature_cols: list[str] | None = None,
):
    """
    –î–µ–ª–∏–º –Ω–∞ X, y –∏ —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è LGBM.
    """
    assert TARGET_COL in df.columns, f"–ù–µ –Ω–∞–π–¥–µ–Ω target_col={TARGET_COL} –≤ train.csv"

    y = df[TARGET_COL]

    if feature_cols is not None:
        # –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ —Ñ–∏—á–∏ (–ø–æ—Å–ª–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è train/test)
        cols_to_use = [c for c in feature_cols if c in df.columns]
        X = df[cols_to_use].copy()
    else:
        # fallback: —Å—Ç–∞—Ä—ã–π –≤–∞—Ä–∏–∞–Ω—Ç —á–µ—Ä–µ–∑ DROP_COLS
        X = df.drop(columns=DROP_COLS + [WEIGHT_COL], errors="ignore")

    feature_names = X.columns.tolist()
    cat_feature_indices = [feature_names.index(c) for c in cat_features if c in feature_names]

    # üëâ –≤—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("\n–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ X (–º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤):")
    print(X.head())

    return X, y, cat_feature_indices, feature_names


def make_lgb_dataset(X, y, cat_feature_indices, weights=None):
    return lgb.Dataset(
        X,
        label=y,
        weight=weights,
        feature_name=list(X.columns),
        categorical_feature=cat_feature_indices,
        free_raw_data=False,
    )


# =========================
#   –ö–ê–°–¢–û–ú–ù–ê–Ø –ú–ï–¢–†–ò–ö–ê WMAE
# =========================

def lgb_wmae(y_pred: np.ndarray, dataset: lgb.Dataset):
    y_true = dataset.get_label()
    w = dataset.get_weight()
    if w is None:
        w = np.ones_like(y_true)

    error = np.abs(y_true - y_pred)
    wmae_value = np.sum(w * error) / np.sum(w)
    # False ‚Äî —á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –ª—É—á—à–µ
    return "wmae", wmae_value, False


# =========================
#   –ü–ê–†–ê–ú–ï–¢–†–´ LGBM
# =========================

def get_lgb_params():
    params = {
        # core
        "objective": "regression_l1",  # MAE –±–ª–∏–∂–µ –∫ WMAE
        "metric": "None",              # –º–µ—Ç—Ä–∏–∫—É —Å—á–∏—Ç–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω–æ–π (WMAE)
        "boosting": "gbdt",
        "num_iterations": 10000,       # –º–∞–∫—Å–∏–º—É–º, –±—É–¥–µ–º —Ä–µ–∑–∞—Ç—å –ø–æ early stopping
        "learning_rate": 0.03,
        "num_leaves": 64,
        "max_depth": -1,

        # —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –∏ –∫–æ–Ω—Ç—Ä–æ–ª—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        "min_data_in_leaf": 50,
        "min_sum_hessian_in_leaf": 1e-3,
        "lambda_l1": 0.0,
        "lambda_l2": 5.0,
        "min_gain_to_split": 0.0,
        "feature_fraction": 0.8,
        "feature_fraction_bynode": 0.8,
        "bagging_fraction": 0.8,
        "bagging_freq": 1,
        "extra_trees": False,

        # —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ
        "force_col_wise": True,   # –º–Ω–æ–≥–æ —Ñ–∏—á ‚Üí col-wise –≤—ã–≥–æ–¥–Ω–µ–µ
        "num_threads": 0,
        "seed": RANDOM_STATE,
        "data_random_seed": RANDOM_STATE,
        "feature_fraction_seed": RANDOM_STATE,
        "bagging_seed": RANDOM_STATE,

        # —É—Ä–æ–≤–µ–Ω—å –ª–æ–≥–æ–≤ (–∞–Ω–∞–ª–æ–≥ verbose –≤ —Ç–≤–æ—ë–º –ø—Ä–∏–º–µ—Ä–µ)
        "verbose": 1,             # <0: fatal, 0: –æ—à–∏–±–∫–∏, 1: info, >1: debug

        # GPU / CPU
        "device_type": "gpu",     # –µ—Å–ª–∏ –Ω–µ—Ç GPU, –ø–æ–º–µ–Ω—è–π –Ω–∞ "cpu"
        "max_bin": 255,
    }
    return params

def get_lgb_param_grid():
    """
    –ù–µ–±–æ–ª—å—à–æ–π –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π –≥—Ä–∏–¥ –≤–æ–∫—Ä—É–≥ –±–∞–∑–æ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
    –ú–æ–∂–Ω–æ –ø–æ—Ç–æ–º —Ä–∞—Å—à–∏—Ä–∏—Ç—å/—Å—É–∑–∏—Ç—å.
    """
    grid = [
        # –±–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (—á—Ç–æ-—Ç–æ –±–ª–∏–∑–∫–æ–µ –∫ —Ç–µ–∫—É—â–µ–π)
        {
            "name": "baseline",
            "learning_rate": 0.03,
            "num_leaves": 64,
            "min_data_in_leaf": 50,
            "lambda_l2": 5.0,
            "feature_fraction": 0.8,
            "bagging_fraction": 0.8,
            "num_iterations": 8000,
        },
        # —á—É—Ç—å —à–∏—Ä–µ –¥–µ—Ä–µ–≤—å—è, –±–æ–ª—å—à–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è, –ø–æ–±–æ–ª—å—à–µ –∫–æ–ª–æ–Ω–æ–∫ –≤ –¥–µ—Ä–µ–≤–µ
        {
            "name": "wide_reg",
            "learning_rate": 0.03,
            "num_leaves": 96,
            "min_data_in_leaf": 40,
            "lambda_l2": 10.0,
            "feature_fraction": 0.9,
            "bagging_fraction": 0.9,
            "num_iterations": 9000,
        },
        # –º–µ–Ω—å—à–∏–π lr, –±–æ–ª—å—à–µ –ª–∏—Å—Ç—å–µ–≤, —Å–∏–ª—å–Ω–µ–µ L2 ‚Äî –∫–ª–∞—Å—Å–∏–∫–∞
        {
            "name": "lr_002_big_leaves",
            "learning_rate": 0.02,
            "num_leaves": 128,
            "min_data_in_leaf": 40,
            "lambda_l2": 12.0,
            "feature_fraction": 0.9,
            "bagging_fraction": 0.9,
            "num_iterations": 12000,
        },
        # –Ω–µ–º–Ω–æ–≥–æ –±–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
        {
            "name": "lr_002_more_regular",
            "learning_rate": 0.02,
            "num_leaves": 96,
            "min_data_in_leaf": 60,
            "lambda_l2": 15.0,
            "feature_fraction": 0.85,
            "bagging_fraction": 0.8,
            "num_iterations": 12000,
        },
        # –µ—â—ë –±–æ–ª–µ–µ –º–∞–ª–µ–Ω—å–∫–∏–π lr, –º–Ω–æ–≥–æ –¥–µ—Ä–µ–≤—å–µ–≤ ‚Äî –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–Ω–æ–≥–æ, —ç—Ç–æ —á–∞—Å—Ç–æ —Ç–æ–ø
        {
            "name": "lr_0015_deep",
            "learning_rate": 0.015,
            "num_leaves": 160,
            "min_data_in_leaf": 60,
            "lambda_l2": 18.0,
            "feature_fraction": 0.9,
            "bagging_fraction": 0.85,
            "num_iterations": 15000,
        },
    ]
    return grid


def tune_lgb_params(df: pd.DataFrame, feature_cols: list[str]):
    """
    –ü–µ—Ä–µ–±–∏—Ä–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π LGBM, –¥–ª—è –∫–∞–∂–¥–æ–π —Å—á–∏—Ç–∞–µ—Ç 5-fold WMAE,
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.
    """
    assert TARGET_COL in df.columns, "–í train –Ω–µ—Ç target"

    # –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏
    cat_features = get_categorical_features(df)

    # —Ñ–æ—Ä–º–∏—Ä—É–µ–º X, y, –≤–µ—Å–∞
    cols_to_use = [c for c in feature_cols if c in df.columns]
    X = df[cols_to_use].copy()
    y = df[TARGET_COL].values
    weights = df[WEIGHT_COL].values

    feature_names = X.columns.tolist()
    cat_feature_indices = [feature_names.index(c) for c in cat_features if c in feature_names]

    print("\n[–¢–Æ–ù–ò–ù–ì] –†–∞–∑–º–µ—Ä X:", X.shape)

    kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)
    base_params = get_lgb_params()
    grid = get_lgb_param_grid()

    best_score = float("inf")
    best_params = None

    for cfg in grid:
        params = base_params.copy()
        params.update(cfg)
        name = params.pop("name", "cfg")

        fold_scores = []

        print(f"\n=== –¢–µ—Å—Ç–∏—Ä—É—é –∫–æ–Ω—Ñ–∏–≥: {name} ===")
        print({k: params[k] for k in ["learning_rate", "num_leaves", "min_data_in_leaf",
                                      "lambda_l2", "feature_fraction", "bagging_fraction",
                                      "num_iterations"]})

        for fold, (train_idx, valid_idx) in enumerate(kf.split(X), 1):
            X_train, y_train = X.iloc[train_idx], y[train_idx]
            X_valid, y_valid = X.iloc[valid_idx], y[valid_idx]
            w_train = weights[train_idx]
            w_valid = weights[valid_idx]

            dtrain = make_lgb_dataset(X_train, y_train, cat_feature_indices, w_train)
            dvalid = make_lgb_dataset(X_valid, y_valid, cat_feature_indices, w_valid)

            model = lgb.train(
                params,
                train_set=dtrain,
                valid_sets=[dvalid],
                valid_names=["valid"],
                feval=lgb_wmae,
                num_boost_round=params["num_iterations"],
                callbacks=[
                    lgb.early_stopping(
                        stopping_rounds=300,
                        first_metric_only=True,
                        verbose=False,
                    ),
                    lgb.log_evaluation(period=0),  # –±–µ–∑ —Å–ø–∞–º–∞
                ],
            )

            score = model.best_score["valid"]["wmae"]
            fold_scores.append(score)

        mean_score = float(np.mean(fold_scores))
        std_score = float(np.std(fold_scores))

        print(f"[{name}] Mean WMAE: {mean_score:.3f} ¬± {std_score:.3f}")

        if mean_score < best_score:
            best_score = mean_score
            best_params = params.copy()
            print(f"--> –ù–æ–≤—ã–π –ª—É—á—à–∏–π –∫–æ–Ω—Ñ–∏–≥: {name} (WMAE={best_score:.3f})")

    print("\n=== –õ–£–ß–®–ò–ï –ù–ê–ô–î–ï–ù–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ ===")
    print(best_params)
    print(f"CV WMAE: {best_score:.3f}")

    return best_params


# =========================
#   –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò
# =========================

def train_lgb_model(df: pd.DataFrame, feature_cols: list[str] | None = None, params: dict | None = None):
    """
    –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö.
    """
    assert TARGET_COL in df.columns, f"–ù–µ –Ω–∞–π–¥–µ–Ω target_col={TARGET_COL} –≤ train.csv"

    if params is None:
        params = get_lgb_params()

    # 1) –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏
    cat_features = get_categorical_features(df)

    # 2) X, y
    if feature_cols is not None:
        cols_to_use = [c for c in feature_cols if c in df.columns]
        X = df[cols_to_use].copy()
    else:
        X = df.drop(columns=DROP_COLS + [WEIGHT_COL], errors="ignore")

    y = df[TARGET_COL].values
    weights = df[WEIGHT_COL].values

    feature_names = X.columns.tolist()
    cat_feature_indices = [feature_names.index(c) for c in cat_features if c in feature_names]

    print("\n[TRAIN] –†–∞–∑–º–µ—Ä X:", X.shape)
    print("–ö–æ–ª-–≤–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Ñ–∏—á–µ–π:", len(cat_feature_indices))

    kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)

    models = []
    fold_scores = []

    for fold, (train_idx, valid_idx) in enumerate(kf.split(X), 1):
        print(f"\n=== Fold {fold} ===")
        X_train, y_train = X.iloc[train_idx], y[train_idx]
        X_valid, y_valid = X.iloc[valid_idx], y[valid_idx]

        w_train = weights[train_idx]
        w_valid = weights[valid_idx]

        dtrain = make_lgb_dataset(X_train, y_train, cat_feature_indices, w_train)
        dvalid = make_lgb_dataset(X_valid, y_valid, cat_feature_indices, w_valid)

        model = lgb.train(
            params,
            train_set=dtrain,
            valid_sets=[dvalid],
            valid_names=["valid"],
            feval=lgb_wmae,
            num_boost_round=params["num_iterations"],
            callbacks=[
                lgb.early_stopping(
                    stopping_rounds=300,
                    first_metric_only=True,
                    verbose=True,
                ),
                lgb.log_evaluation(period=100),
            ],
        )

        models.append(model)
        best_wmae = model.best_score["valid"]["wmae"]
        fold_scores.append(best_wmae)
        print(f"Fold {fold} best WMAE: {best_wmae:.6f}")

    print(f"\nMean WMAE across folds: {np.mean(fold_scores):.6f} ¬± {np.std(fold_scores):.6f}")
    return models, feature_names, cat_feature_indices

# =========================
#   –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø –ù–ê TEST
# =========================

def predict_test(models, feature_names, test_path=TEST_PATH):
    test_df = load_data(test_path)
    test_df = clean_object_columns(test_df)

    X_test = test_df[feature_names]

    preds = np.zeros(len(X_test))
    for model in models:
        preds += model.predict(X_test, num_iteration=model.best_iteration)
    preds /= len(models)

    submission = pd.DataFrame({
        "id": test_df["id"],
        TARGET_COL: preds,
    })

    submission.to_csv("submission.csv", index=False)
    print("submission.csv —Å–æ—Ö—Ä–∞–Ω—ë–Ω")
    return submission


def clean_object_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤:
    - –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ‚Äî —Å—Ç—Ä–æ–≥–æ EXPLICIT_CATEGORY_COLS
    - –æ—Å—Ç–∞–ª—å–Ω—ã–µ object ‚Äî –ø—ã—Ç–∞–µ–º—Å—è –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —á–∏—Å–ª–∞–º
    """
    df = df.copy()

    for col in df.columns:
        if col in EXPLICIT_CATEGORY_COLS:
            df[col] = df[col].astype("category")
            continue

        if df[col].dtype == "object":
            # –ø—Ä–æ–±—É–µ–º –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫–æ–ª–æ–Ω–∫—É –∫ —á–∏—Å–ª—É
            s = (
                df[col]
                .astype(str)
                .str.strip()
                .replace({"nan": np.nan, "NaN": np.nan, "None": np.nan, "": np.nan})
            )

            # –ø—Ä–æ–±—É–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ float
            s_num = pd.to_numeric(
                s.str.replace(" ", "", regex=False).str.replace(",", ".", regex=False),
                errors="coerce",
            )

            df[col] = s_num  # –¥–∞–∂–µ –µ—Å–ª–∏ —á–∞—Å—Ç—å NaN ‚Äî —ç—Ç–æ –æ–∫

    return df


# =========================
#   MAIN
# =========================

def train():
    print("–ó–∞–≥—Ä—É–∂–∞—é hackathon_income_train.csv‚Ä¶")
    train_df = load_data(TRAIN_PATH)

    print("–ó–∞–≥—Ä—É–∂–∞—é hackathon_income_test.csv‚Ä¶")
    test_df = load_data(TEST_PATH)

    train_df = clean_object_columns(train_df)
    test_df = clean_object_columns(test_df)

    # —Å–µ—Ä–≤–∏—Å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ñ–∏—á–∞–º–∏
    service_cols = ["id", "dt", TARGET_COL, WEIGHT_COL]  # —É —Ç–µ–±—è –≤–µ—Å –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è wmae_weight

    train_cols = set(train_df.columns)
    test_cols = set(test_df.columns)

    feature_cols = sorted((train_cols & test_cols) - set(service_cols))

    print("\n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á:", len(feature_cols))
    print("–ü–µ—Ä–≤—ã–µ 20 —Ñ–∏—á:", feature_cols[:20])

    print("\n–¢–∏–ø—ã –ø–µ—Ä–≤—ã—Ö 20 –∫–æ–ª–æ–Ω–æ–∫ train:")
    print(train_df.dtypes.head(20))
    print("\n–ï—Å—Ç—å –ª–∏ 'target'?", TARGET_COL in train_df.columns)

        # 1) –ø–æ–¥–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ CV
    print("\n=== –¢—é–Ω–∏–Ω–≥ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ LightGBM –ø–æ CV (WMAE) ===")
    best_params = tune_lgb_params(train_df, feature_cols)

    # 2) –æ–±—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö
    print("\n=== –û–±—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö ===")
    models, feature_names, cat_feature_indices = train_lgb_model(train_df, feature_cols, params=best_params)


    print("\n=== –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ ===")
    print("–°–æ—Ö—Ä–∞–Ω—è—é –ø–µ—Ä–≤—É—é –º–æ–¥–µ–ª—å ‚Üí model.txt")
    models[0].save_model("model.txt")


if __name__ == "__main__":
    train()
